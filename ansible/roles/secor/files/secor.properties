# Regular expression matching names of consumed topics.
secor.kafka.topic_filter=.*
secor.kafka.topic_blacklist=

cloud.service=S3

aws.access.key=
aws.secret.key=
aws.role=
aws.proxy.isEnabled=false
aws.sse.type=
aws.sse.customer.key=
aws.sse.kms.key=
aws.region=us-east-1
aws.endpoint=
aws.client.pathstyleaccess=true

secor.s3.filesystem=s3a

zookeeper.session.timeout.ms=3000
zookeeper.sync.time.ms=200
secor.zookeeper.path=/
kafka.consumer.timeout.ms=10000
kafka.consumer.auto.offset.reset=smallest
kafka.partition.assignment.strategy=range
kafka.rebalance.max.retries=
kafka.rebalance.backoff.ms=
kafka.socket.receive.buffer.bytes=
kafka.fetch.message.max.bytes=
kafka.fetch.min.bytes=
kafka.fetch.wait.max.ms=
kafka.seed.broker.port=9092
kafka.zookeeper.path=/
kafka.dual.commit.enabled=true
kafka.offsets.storage=zookeeper
kafka.useTimestamp=false

secor.generation=1
secor.consumer.threads=1
secor.messages.per.second=10000
secor.offsets.per.partition=10000000
secor.offsets.prefix=offset=
secor.topic_partition.forget.seconds=600

partitioner.granularity.hour=true
partitioner.granularity.minute=false
partitioner.granularity.date.prefix=dt=
partitioner.granularity.hour.prefix=hr=
partitioner.granularity.minute.prefix=min=
partitioner.granularity.date.format=yyyy-MM-dd
partitioner.granularity.hour.format=HH
partitioner.granularity.minute.format=mm
partitioner.finalizer.delay.seconds=60

secor.finalizer.lookback.periods=10
qubole.api.token=
hive.table.prefix=
tsdb.hostport=
monitoring.blacklist.topics=
monitoring.prefix=secor
monitoring.interval.seconds=0
statsd.hostport=
secor.thrift.protocol.class=
secor.thrift.message.class.*=
statsd.prefixWithConsumerGroup=true

message.timestamp.name=timestamp
message.timestamp.name.separator=
message.timestamp.id=1
message.timestamp.type=i64
message.timestamp.input.pattern=
message.timestamp.required=true
secor.compression.codec=org.apache.hadoop.io.compress.GzipCodec
secor.file.extension=.gz


secor.file.reader.writer.factory=com.pinterest.secor.io.impl.DelimitedTextFileReaderWriterFactory

secor.file.reader.Delimiter=\n
secor.file.writer.Delimiter=\n

secor.max.message.size.bytes=100000
secor.upload.manager.class=com.pinterest.secor.uploader.S3UploadManager

secor.parser.timezone=UTC
secor.message.transformer.class=com.pinterest.secor.transformer.IdentityMessageTransformer
secor.s3.prefix.md5hash=false
secor.s3.alter.path.date=
secor.s3.alternative.path=

secor.enable.qubole=true
secor.qubole.timeout.ms=300000
secor.kafka.upload_at_minute_mark.topic_filter=
secor.upload.minute_mark=0
secor.file.age.youngest=true
secor.monitoring.metrics.collector.class=com.pinterest.secor.monitoring.OstrichMetricCollector

parquet.block.size=134217728
parquet.page.size=1048576
parquet.enable.dictionary=true
parquet.validation=false

secor.orc.message.schema.*=struct<a:int\,b:int\,c:struct<d:int\,e:string>\,f:array<string>\,g:int>
secor.orc.schema.provider=com.pinterest.secor.util.orc.schema.DefaultORCSchemaProvider

kafka.seed.broker.host=localhost
kafka.seed.broker.port=9092

zookeeper.quorum=localhost:2181

secor.max.file.size.bytes=10000
secor.max.file.age.seconds=10

secor.kafka.group=secor_backup

secor.message.parser.class=com.pinterest.secor.parser.Iso8601MessageParser

secor.s3.path=secor

# # Swift path where sequence files are stored.
# secor.swift.path=

# Local path where sequence files are stored before they are uploaded to s3.
secor.local.path=/opt/secor/backup

# If greater than 0, upon startup Secor will clean up directories and files under secor.local.path
# that are older than this value.
secor.local.log.delete.age.hours=12

# Port of the Ostrich server.
ostrich.port=9999

secor.s3.bucket=btrx
